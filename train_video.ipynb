{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"TFA-GAMMA","language":"python","name":"tf-max-tfa-gamma"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"train_video.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"skT5x5ikdd7f"},"source":["# DCASE-2020 Video Subnetwork"]},{"cell_type":"markdown","metadata":{"id":"g4QuXX-ddd7i"},"source":["Author: Maximo Cobos\n"]},{"cell_type":"code","metadata":{"id":"yLsOy4grdd7i"},"source":["# Import necessary standard packages\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-I_FEU-Wdd7j"},"source":["## Input Data"]},{"cell_type":"markdown","metadata":{"id":"TpMX-0BIdd7k"},"source":["Specify path to folder containing the video dataset and the output path for the tfrecords:"]},{"cell_type":"code","metadata":{"id":"d2E-0AwKdd7k","outputId":"e1da59b5-e24a-48e2-9512-b027198b1c45"},"source":["# TFRecords folder\n","main_dir = '.\\\\tfrecords_gamma'\n","root_path = Path(main_dir)\n","\n","# Train Fold\n","train_fold_path = '.\\\\dataset\\\\evaluation_setup\\\\fold1_train.csv'\n","traindf = pd.read_csv(train_fold_path, sep='\\t', lineterminator='\\r')\n","trainlist = traindf[traindf.columns[1]].tolist()\n","trainfiles = [Path(f).with_suffix('.tfrecords').name for f in trainlist]\n","\n","# Validation Fold\n","val_fold_path = '.\\\\dataset\\\\evaluation_setup\\\\fold1_test.csv'\n","valdf = pd.read_csv(val_fold_path, sep='\\t', lineterminator='\\r')\n","vallist = valdf[valdf.columns[1]].tolist()\n","valfiles = [Path(f).with_suffix('.tfrecords').name for f in vallist]\n","\n","len(trainfiles), len(valfiles)"],"execution_count":null,"outputs":[{"data":{"text/plain":["(8646, 3645)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"S0UXuX94dd7l","outputId":"22946196-b43d-460b-a74b-e6f79c534948"},"source":["def get_label(filepath):\n","    '''Receives a path to a video and returns its label\n","    '''\n","    scn_dict = {'airport': 0, 'shopping_mall': 1, 'metro_station': 2, \n","                'street_pedestrian': 3, 'public_square': 4, 'street_traffic': 5,\n","                'tram': 6, 'bus': 7, 'metro': 8, 'park': 9}\n","    \n","    fileid = Path(filepath).name\n","    scn_id = fileid.split('-')[0]\n","    label = scn_dict[scn_id]\n","    return label\n","\n","# Get labels\n","train_labels = [get_label(f) for f in trainfiles]\n","val_labels = [get_label(f) for f in valfiles]\n","\n","trainfiles = [main_dir + '\\\\' + str(label) + '\\\\' + f for f,label in zip(trainfiles,train_labels)]\n","valfiles = [main_dir + '\\\\' + str(label) + '\\\\' + f for f,label in zip(valfiles,val_labels)]\n","\n","N_val = len(valfiles)\n","\n","# Get number of examples per class\n","num_class_ex = []\n","for i in range(10):\n","    num_class_ex.append(train_labels.count(i))     \n","\n","\n","# Get class weights    \n","N_train = len(train_labels)\n","num_classes = 10\n","class_weights = []\n","for i in range(num_classes):\n","    weight = ( 1 / num_class_ex[i]) * N_train / num_classes\n","    class_weights.append(weight)  \n","\n","keylst = np.arange(0,len(class_weights))\n","class_weights = {keylst[i]: class_weights[i] for i in range(0, len(class_weights))}    \n","\n","plt.bar(class_weights.keys(), class_weights.values())\n","plt.title('Class Weights');\n"],"execution_count":null,"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"id":"TnF7DxH5dd7m"},"source":["### Parsing function"]},{"cell_type":"code","metadata":{"id":"dGjxh9XVdd7m"},"source":["def parse_sequence(sequence_example, avmode = 'audiovideo'):\n","    \"\"\"this function is the sequence parser for the created TFRecords file\"\"\"\n","    \n","    sequence_features = {'VideoFrames': tf.io.FixedLenSequenceFeature([], dtype=tf.string),                    \n","                         'Labels': tf.io.FixedLenSequenceFeature([], dtype=tf.int64)}\n","\n","    context_features = {'AudioFrames': tf.io.FixedLenFeature((96000,), dtype=tf.float32),\n","                        'length': tf.io.FixedLenFeature([], dtype=tf.int64)}\n","    context, sequence = tf.io.parse_single_sequence_example(\n","    sequence_example, context_features=context_features, sequence_features=sequence_features)\n","\n","    # get features context\n","    seq_length = tf.cast(context['length'], dtype = tf.int32)   \n","\n","    # decode video and audio\n","    video = tf.io.decode_raw(sequence['VideoFrames'], tf.uint8)\n","    video = tf.reshape(video, shape=(seq_length, 224, 224, 3))\n","    audio = tf.cast(context['AudioFrames'], tf.float32)\n","    audio = tf.reshape(audio, shape=(64, 500, 3))\n","    label = tf.cast(sequence['Labels'], dtype = tf.int32)\n","        \n","    video = tf.cast(video, tf.float32)\n","    \n","    if avmode == 'audio':\n","        return audio, label\n","    elif avmode == 'video':\n","        return video, label\n","    elif avmode == 'audiovideo':\n","        return video, audio, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7C41Zyhdd7n","outputId":"0b15d715-d83c-45c7-febf-7a39f5a26e22"},"source":["# Check parsing function\n","filesds = tf.data.Dataset.from_tensor_slices(trainfiles)\n","dataset = tf.data.TFRecordDataset(filesds)\n","dataset = dataset.map(lambda tf_file: parse_sequence(tf_file,'video'), num_parallel_calls=4)\n","datait = iter(dataset)\n","example = datait.get_next()\n","print(example[0].shape, example[1].shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(50, 224, 224, 3) (50,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"XQnxlVdwdd7n"},"source":["## Video Pipeline"]},{"cell_type":"code","metadata":{"id":"pZN01JZydd7o"},"source":["def random_cut_video(video):\n","    ''' Extract randomly one second of video (5 frames)\n","    '''\n","    seq_length = tf.shape(video)[0]\n","    min_v = 0\n","    max_v = seq_length - 5\n","    rnum = tf.random.uniform([1], minval=min_v, maxval=max_v, dtype=tf.dtypes.int32)\n","    video = video[rnum[0]:rnum[0]+5,...]\n","    return video\n","\n","def process_ds_video(video,label):\n","    ''' Process video in training\n","    '''\n","    video = random_cut_video(video)\n","    label = label[0]    \n","    label = tf.one_hot(label,10)\n","    return video, label\n","\n","def process_ds_video_val(video,label):\n","    ''' Process video in validation (reshapes a 10 second file into 10 different examples)\n","    '''\n","    video = tf.reshape(video, shape=[10,5,224,224,3])\n","    label = label[0:10]\n","    label = tf.one_hot(label,10)\n","    return video, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YCML9S9dd7o"},"source":["# Generate Train tf.data.dataset\n","train_batch_size = 16\n","\n","trainds = tf.data.Dataset.from_tensor_slices(trainfiles)\n","trainds = trainds.shuffle(N_train)\n","trainds = trainds.repeat()  \n","    \n","trainds = tf.data.TFRecordDataset(trainds)\n","trainds = trainds.map(lambda tf_file: parse_sequence(tf_file,'video'), num_parallel_calls=4)\n","trainds = trainds.map(lambda video, label: process_ds_video(video, label), num_parallel_calls=4)\n","trainds = trainds.batch(train_batch_size)  \n","\n","# Generate Validation dataset\n","\n","#val_batch_size = 16\n","\n","valds = tf.data.Dataset.from_tensor_slices(valfiles)\n","valds = tf.data.TFRecordDataset(valds)\n","valds = valds.map(lambda tf_file: parse_sequence(tf_file,'video'), num_parallel_calls=4)\n","valds = valds.map(lambda video, label: process_ds_video_val(video, label), num_parallel_calls=4)\n","#valds = valds.batch(val_batch_size)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_T1cV8lFdd7o","outputId":"31b3e0fe-ad77-4f2c-e130-4a7c47af28b3"},"source":["datait = iter(trainds)\n","example = datait.get_next()\n","print(example[0].shape, example[1].shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(16, 5, 224, 224, 3) (16, 10)\n"]}]},{"cell_type":"code","metadata":{"id":"bKjGrHSNdd7o","outputId":"fd6557a1-243d-41a5-880e-5b6ea79f276f"},"source":["datait = iter(valds)\n","example = datait.get_next()\n","print(example[0].shape, example[1].shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 5, 224, 224, 3) (10, 10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"cqAbg4_9dd7p"},"source":["## Create model"]},{"cell_type":"code","metadata":{"id":"T9JW8x_kdd7p"},"source":["from tensorflow.keras.layers import Input, Activation, Conv2D, Flatten, BatchNormalization\n","from tensorflow.keras.layers import Dropout, SpatialDropout1D, Dense, SpatialDropout2D\n","from tensorflow.keras.layers import TimeDistributed, GRU, Bidirectional\n","from tensorflow.keras.layers import GlobalAveragePooling1D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.models import Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21X2vrLXdd7p"},"source":["from tensorflow.keras.models import save_model, load_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLDmXA8idd7p"},"source":["### First step: Create Time-Distributed VGG16 trained on Places365"]},{"cell_type":"markdown","metadata":{"id":"B1HTvGIgdd7p"},"source":["First step: Load pre-trained model"]},{"cell_type":"code","metadata":{"id":"WdWpqAwodd7p"},"source":["pre_model = load_model('PATH_TO\\vgg16_places_t2_1x.hdf5', compile=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5IzoN0lfdd7p"},"source":["Second step: Make the equivalent Time-Distributed model and transfer weights"]},{"cell_type":"code","metadata":{"id":"_cleES2pdd7p","outputId":"29c1b757-dbc0-462e-cde3-bc1eb1620ec6"},"source":["regularization = l2(0.001)    \n","\n","num_classes = 10 \n","input_shape = (None,224,224,3)\n","input_vid = Input(shape = input_shape)\n","\n","# Block 1\n","x = TimeDistributed(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block1_conv1')(input_vid)\n","\n","x = TimeDistributed(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block1_conv2')(x)\n","\n","x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), name=\"block1_pool\")(x)\n","\n","# Block 2\n","x = TimeDistributed(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block2_conv1')(x)\n","\n","x = TimeDistributed(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block2_conv2')(x)\n","\n","x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), name=\"block2_pool\")(x)\n","\n","# Block 3\n","x = TimeDistributed(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block3_conv1')(x)\n","\n","x = TimeDistributed(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block3_conv2')(x)\n","\n","x = TimeDistributed(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block3_conv3')(x)\n","\n","x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), name=\"block3_pool\")(x)\n","\n","# Block 4\n","x = TimeDistributed(Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block4_conv1')(x)\n","\n","x = TimeDistributed(Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block4_conv2')(x)\n","\n","x = TimeDistributed(Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block4_conv3')(x)\n","\n","x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), name=\"block4_pool\")(x)\n","\n","# Block 5\n","x = TimeDistributed(Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block5_conv1')(x)\n","\n","x = TimeDistributed(Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block5_conv2')(x)\n","\n","x = TimeDistributed(Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n","           kernel_regularizer=l2(0.0002),\n","           activation='relu'), name='block5_conv3')(x)\n","\n","x = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), name=\"block5_pool\")(x)\n","\n","\n","vggpre = Model(inputs=input_vid, outputs=x)  \n","vggpre.load_weights('PATH_TO\\vgg16_places_t2_1x.hdf5', by_name = False)\n","\n","# Freeze all weights\n","\n","for layer in vggpre.layers:\n","    print('Setting layer {} non-trainable'.format(layer.name))\n","    layer.trainable = False\n","\n","vggpre.summary()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Setting layer input_2 non-trainable\n","Setting layer block1_conv1 non-trainable\n","Setting layer block1_conv2 non-trainable\n","Setting layer block1_pool non-trainable\n","Setting layer block2_conv1 non-trainable\n","Setting layer block2_conv2 non-trainable\n","Setting layer block2_pool non-trainable\n","Setting layer block3_conv1 non-trainable\n","Setting layer block3_conv2 non-trainable\n","Setting layer block3_conv3 non-trainable\n","Setting layer block3_pool non-trainable\n","Setting layer block4_conv1 non-trainable\n","Setting layer block4_conv2 non-trainable\n","Setting layer block4_conv3 non-trainable\n","Setting layer block4_pool non-trainable\n","Setting layer block5_conv1 non-trainable\n","Setting layer block5_conv2 non-trainable\n","Setting layer block5_conv3 non-trainable\n","Setting layer block5_pool non-trainable\n","Model: \"model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, None, 224, 224, 3 0         \n","_________________________________________________________________\n","block1_conv1 (TimeDistribute (None, None, 224, 224, 64 1792      \n","_________________________________________________________________\n","block1_conv2 (TimeDistribute (None, None, 224, 224, 64 36928     \n","_________________________________________________________________\n","block1_pool (TimeDistributed (None, None, 112, 112, 64 0         \n","_________________________________________________________________\n","block2_conv1 (TimeDistribute (None, None, 112, 112, 12 73856     \n","_________________________________________________________________\n","block2_conv2 (TimeDistribute (None, None, 112, 112, 12 147584    \n","_________________________________________________________________\n","block2_pool (TimeDistributed (None, None, 56, 56, 128) 0         \n","_________________________________________________________________\n","block3_conv1 (TimeDistribute (None, None, 56, 56, 256) 295168    \n","_________________________________________________________________\n","block3_conv2 (TimeDistribute (None, None, 56, 56, 256) 590080    \n","_________________________________________________________________\n","block3_conv3 (TimeDistribute (None, None, 56, 56, 256) 590080    \n","_________________________________________________________________\n","block3_pool (TimeDistributed (None, None, 28, 28, 256) 0         \n","_________________________________________________________________\n","block4_conv1 (TimeDistribute (None, None, 28, 28, 512) 1180160   \n","_________________________________________________________________\n","block4_conv2 (TimeDistribute (None, None, 28, 28, 512) 2359808   \n","_________________________________________________________________\n","block4_conv3 (TimeDistribute (None, None, 28, 28, 512) 2359808   \n","_________________________________________________________________\n","block4_pool (TimeDistributed (None, None, 14, 14, 512) 0         \n","_________________________________________________________________\n","block5_conv1 (TimeDistribute (None, None, 14, 14, 512) 2359808   \n","_________________________________________________________________\n","block5_conv2 (TimeDistribute (None, None, 14, 14, 512) 2359808   \n","_________________________________________________________________\n","block5_conv3 (TimeDistribute (None, None, 14, 14, 512) 2359808   \n","_________________________________________________________________\n","block5_pool (TimeDistributed (None, None, 7, 7, 512)   0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"c5msBONidd7q"},"source":["Third step: Add recurrent part"]},{"cell_type":"code","metadata":{"id":"G3btp4CXdd7q","outputId":"351904d9-8ecb-44fd-afa4-ac462d926670"},"source":["\n","x = TimeDistributed(GlobalMaxPooling2D(), name='TD_C_GlobAvPooling2D')(vggpre.layers[-1].output)\n","\n","# Recurrent Block\n","fw = GRU(32, return_sequences=True, stateful=False, recurrent_dropout = 0.0, name='VID_RNN_fw')\n","bw = GRU(32, return_sequences=True, stateful=False, recurrent_dropout = 0.0, go_backwards=True, name='VID_RNN_bw')\n","x = Bidirectional(fw, backward_layer=bw, name='VID_RNN_bidir')(x) \n","x = Dropout(0.5, name='VID_C2_Dropout')(x) \n","x = Dense(num_classes, kernel_regularizer = regularization, name ='VID_C2_Dense')(x)\n","x = Activation('softmax', name = 'VID_C2_Act_softmax_1')(x)\n","x = GlobalAveragePooling1D(name='VID_Pred')(x)\n","          \n","video_model = Model(inputs=input_vid, outputs=x)  \n","video_model.summary()\n","   "],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, None, 224, 224, 3 0         \n","_________________________________________________________________\n","block1_conv1 (TimeDistribute (None, None, 224, 224, 64 1792      \n","_________________________________________________________________\n","block1_conv2 (TimeDistribute (None, None, 224, 224, 64 36928     \n","_________________________________________________________________\n","block1_pool (TimeDistributed (None, None, 112, 112, 64 0         \n","_________________________________________________________________\n","block2_conv1 (TimeDistribute (None, None, 112, 112, 12 73856     \n","_________________________________________________________________\n","block2_conv2 (TimeDistribute (None, None, 112, 112, 12 147584    \n","_________________________________________________________________\n","block2_pool (TimeDistributed (None, None, 56, 56, 128) 0         \n","_________________________________________________________________\n","block3_conv1 (TimeDistribute (None, None, 56, 56, 256) 295168    \n","_________________________________________________________________\n","block3_conv2 (TimeDistribute (None, None, 56, 56, 256) 590080    \n","_________________________________________________________________\n","block3_conv3 (TimeDistribute (None, None, 56, 56, 256) 590080    \n","_________________________________________________________________\n","block3_pool (TimeDistributed (None, None, 28, 28, 256) 0         \n","_________________________________________________________________\n","block4_conv1 (TimeDistribute (None, None, 28, 28, 512) 1180160   \n","_________________________________________________________________\n","block4_conv2 (TimeDistribute (None, None, 28, 28, 512) 2359808   \n","_________________________________________________________________\n","block4_conv3 (TimeDistribute (None, None, 28, 28, 512) 2359808   \n","_________________________________________________________________\n","block4_pool (TimeDistributed (None, None, 14, 14, 512) 0         \n","_________________________________________________________________\n","block5_conv1 (TimeDistribute (None, None, 14, 14, 512) 2359808   \n","_________________________________________________________________\n","block5_conv2 (TimeDistribute (None, None, 14, 14, 512) 2359808   \n","_________________________________________________________________\n","block5_conv3 (TimeDistribute (None, None, 14, 14, 512) 2359808   \n","_________________________________________________________________\n","block5_pool (TimeDistributed (None, None, 7, 7, 512)   0         \n","_________________________________________________________________\n","TD_C_GlobAvPooling2D (TimeDi (None, None, 512)         0         \n","_________________________________________________________________\n","VID_RNN_bidir (Bidirectional (None, None, 64)          104832    \n","_________________________________________________________________\n","VID_C2_Dropout (Dropout)     (None, None, 64)          0         \n","_________________________________________________________________\n","VID_C2_Dense (Dense)         (None, None, 10)          650       \n","_________________________________________________________________\n","VID_C2_Act_softmax_1 (Activa (None, None, 10)          0         \n","_________________________________________________________________\n","VID_Pred (GlobalAveragePooli (None, 10)                0         \n","=================================================================\n","Total params: 14,820,170\n","Trainable params: 105,482\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"8qoFZq1tdd7q"},"source":["## Train Model"]},{"cell_type":"code","metadata":{"id":"afUcK34bdd7r"},"source":["# Compile model\n","learning_rate = 0.0001\n","opt = tf.keras.optimizers.get('adam')\n","tf.keras.backend.set_value(opt.learning_rate, learning_rate)\n","\n","video_model.compile(\n","    loss = {'VID_Pred': 'categorical_crossentropy'},\n","    optimizer=opt,            \n","    metrics = {'VID_Pred': 'accuracy'},\n",")      \n","\n","from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","import os\n","callbacks = []\n","ckpt_dir = 'PATH_WHERE\\checkpoints'\n","model_name = 'video_final'\n","callbacks.append(\n","            ModelCheckpoint(\n","                filepath=os.path.join(ckpt_dir, '%s-{epoch:02d}-{val_accuracy:.2f}.hdf5' % model_name),\n","                monitor=\"val_accuracy\",\n","                mode=\"max\",\n","                save_best_only=True,\n","                save_weights_only=True,\n","                verbose=True,\n","            )\n","        )\n","\n","callbacks.append(\n","                EarlyStopping(\n","                    monitor=\"val_loss\",\n","                    patience=40,\n","                )\n","            )\n","\n","callbacks.append(\n","                ReduceLROnPlateau(\n","                    monitor=\"val_loss\", \n","                    factor=0.5, \n","                    patience=20, \n","                    verbose=True,\n","                )\n","            )\n","\n","callbacks.append(\n","    CSVLogger(                    \n","        filename = os.path.join(ckpt_dir, '%s.csv' % model_name),\n","        append = False,                \n","    )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsE906--dd7r"},"source":["# Train model\n","history = video_model.fit(\n","            trainds,\n","            epochs=200,\n","            steps_per_epoch= int(N_train/train_batch_size),    # Set according to number of examples and training batch size\n","            validation_data = valds,\n","            validation_steps = int(N_val),\n","            callbacks=callbacks,  # Include list of callbacks    \n","            #class_weight = class_weights,    \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4g6cbtTdd7r"},"source":["plt.figure(figsize=(16,5))\n","plt.subplot(1,2,1)\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","\n","plt.subplot(1,2,2)\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjsyZYnhdd7r"},"source":["Note: I trained the above model freezing all pre-trained weights and achieved 0.81% accuracy.\n","In this second try, I unfreeze all layers and lower to a very small learning rate"]},{"cell_type":"code","metadata":{"id":"F79UQRPKdd7r","outputId":"33ee37eb-0cd5-4abd-ee25-5a0c0e5ee824"},"source":["video_model.load_weights('PATH_TO_BEST_VIDEO_MODEL')\n","for layer in video_model.layers:\n","    print('Setting layer {} non-trainable'.format(layer.name))\n","    layer.trainable = True"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Setting layer input_2 non-trainable\n","Setting layer block1_conv1 non-trainable\n","Setting layer block1_conv2 non-trainable\n","Setting layer block1_pool non-trainable\n","Setting layer block2_conv1 non-trainable\n","Setting layer block2_conv2 non-trainable\n","Setting layer block2_pool non-trainable\n","Setting layer block3_conv1 non-trainable\n","Setting layer block3_conv2 non-trainable\n","Setting layer block3_conv3 non-trainable\n","Setting layer block3_pool non-trainable\n","Setting layer block4_conv1 non-trainable\n","Setting layer block4_conv2 non-trainable\n","Setting layer block4_conv3 non-trainable\n","Setting layer block4_pool non-trainable\n","Setting layer block5_conv1 non-trainable\n","Setting layer block5_conv2 non-trainable\n","Setting layer block5_conv3 non-trainable\n","Setting layer block5_pool non-trainable\n","Setting layer TD_C_GlobAvPooling2D non-trainable\n","Setting layer VID_RNN_bidir non-trainable\n","Setting layer VID_C2_Dropout non-trainable\n","Setting layer VID_C2_Dense non-trainable\n","Setting layer VID_C2_Act_softmax_1 non-trainable\n","Setting layer VID_Pred non-trainable\n"]}]},{"cell_type":"code","metadata":{"id":"5iloBq9edd7s"},"source":["# Compile model\n","learning_rate = 0.000001 # 0.0001 was the original learning rate with frozen weights. I lowered in order not to destroy weights\n","opt = tf.keras.optimizers.get('adam')\n","tf.keras.backend.set_value(opt.learning_rate, learning_rate)\n","\n","video_model.compile(\n","    loss = {'VID_Pred': 'categorical_crossentropy'},\n","    optimizer=opt,            \n","    metrics = {'VID_Pred': 'accuracy'},\n",")      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N91ZT-8ldd7s"},"source":["from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","import os\n","callbacks = []\n","ckpt_dir = 'PATH_WHERE\\checkpoints'\n","model_name = 'video_final_un'\n","callbacks.append(\n","            ModelCheckpoint(\n","                filepath=os.path.join(ckpt_dir, '%s-{epoch:02d}-{val_accuracy:.2f}.hdf5' % model_name),\n","                monitor=\"val_accuracy\",\n","                mode=\"max\",\n","                save_best_only=True,\n","                save_weights_only=True,\n","                verbose=True,\n","            )\n","        )\n","\n","callbacks.append(\n","                EarlyStopping(\n","                    monitor=\"val_loss\",\n","                    patience=40,\n","                )\n","            )\n","\n","callbacks.append(\n","                ReduceLROnPlateau(\n","                    monitor=\"val_loss\", \n","                    factor=0.5, \n","                    patience=20, \n","                    verbose=True,\n","                )\n","            )\n","\n","callbacks.append(\n","    CSVLogger(                    \n","        filename = os.path.join(ckpt_dir, '%s.csv' % model_name),\n","        append = False,                \n","    )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGvO2N04dd7s"},"source":["# Train model\n","history = video_model.fit(\n","            trainds,\n","            epochs=200,\n","            steps_per_epoch= int(N_train/train_batch_size),    # Set according to number of examples and training batch size\n","            validation_data = valds,\n","            validation_steps = int(N_val),\n","            #validation_steps = int(N_val/val_batch_size),\n","            callbacks=callbacks,  # Include list of callbacks    \n","            #class_weight = class_weights,    \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMc-X00Zdd7s"},"source":["model_name = 'video_final_un2'\n","callbacks.append(\n","            ModelCheckpoint(\n","                filepath=os.path.join(ckpt_dir, '%s-{epoch:02d}-{val_accuracy:.2f}.hdf5' % model_name),\n","                monitor=\"val_accuracy\",\n","                mode=\"max\",\n","                save_best_only=True,\n","                save_weights_only=True,\n","                verbose=True,\n","            )\n","        )\n","\n","callbacks.append(\n","                EarlyStopping(\n","                    monitor=\"val_loss\",\n","                    patience=40,\n","                )\n","            )\n","\n","callbacks.append(\n","                ReduceLROnPlateau(\n","                    monitor=\"val_loss\", \n","                    factor=0.5, \n","                    patience=20, \n","                    verbose=True,\n","                )\n","            )\n","\n","callbacks.append(\n","    CSVLogger(                    \n","        filename = os.path.join(ckpt_dir, '%s.csv' % model_name),\n","        append = False,                \n","    )\n",")\n","\n","\n","# Train model\n","history = video_model.fit(\n","            trainds,\n","            epochs=200,\n","            steps_per_epoch= int(N_train/train_batch_size),    # Set according to number of examples and training batch size\n","            validation_data = valds,\n","            validation_steps = int(N_val),\n","            #validation_steps = int(N_val/val_batch_size),\n","            callbacks=callbacks,  # Include list of callbacks    \n","            #class_weight = class_weights,    \n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoGC8ASNdd7s"},"source":["plt.figure(figsize=(16,5))\n","plt.subplot(1,2,1)\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","\n","plt.subplot(1,2,2)\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuLD2Hbzdd7s"},"source":["save_model(video_model, 'VGG16_RCNN_unpvideo3_86.hdf5')"],"execution_count":null,"outputs":[]}]}